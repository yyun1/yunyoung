---
title: "HW8_Yun_Young"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---
```{r, echo = F,  message=F, warning=FALSE}
library(tidyr)
library(dplyr)
library(tokenizers)
library(tidytext)
library(ggplot2)
library(knitr)
library(widyr)

fig <- local({
    i <- 0
    ref <- list()
    list(
        cap=function(refName, text) {
            i <<- i + 1
            ref[[refName]] <<- i
            text
        },
        ref=function(refName) {
            ref[[refName]]
        })
})
```


#Problem 2#

```{r echo = F, warning=FALSE, message=FALSE}
###This code cleans the input data.
###Reads the data in with 'read.delim' function and unifies the little differences in expressing the same word using sub/gsub functions and for-loop. 

url<-"https://raw.githubusercontent.com/rsettlage/STAT_5014/master/08_text_mining_Rnotebooks_bash_sed_awk/survey_data.txt"
surveydata <- read.delim(url, header=T, skip=0, fill=T, stringsAsFactors = F)

surveydata <- data.frame(surveydata)

surveydata$Major <- gsub(pattern = "Math-BS", replacement = "Math", surveydata$Major) %>% gsub(pattern = "Math\\(Stat\\)-BS", replacement = "Math/Stat") %>% gsub(pattern = "Finance-BS, Finance-MS", replacement = "Finance") %>% gsub(pattern = "Finance Engineer BS STAT-Master", replacement = "Finance Engineer/Stat") %>% gsub(pattern = "Math, History", replacement = "Math/History") %>% gsub(pattern = "Econ, Math", replacement = "Econ/Math")

surveydata$Platform <- gsub(pattern = "PC-Surface", replacement = "PC", surveydata$Platform) %>% gsub(pattern = "Mac", replacement = "MAC")

for(i in 1:length(surveydata$R.level))
{
  surveydata$R.level <- tolower(surveydata$R.level)
  if (!(surveydata$R.level[i] == "beginner" || surveydata$R.level[i] == "intermediate"))
  {
    if((surveydata$R.level[i] == "int") || (surveydata$R.level[i] == "intermed"))
    {
      surveydata$R.level[i] <- "intermediate"
    }
    
    if((surveydata$R.level[i] == "beg/intermed") || (surveydata$R.level[i] == "beg/intermediate"))
    {
      surveydata$R.level[i] <- "beginner/intermediate"      
    }
  }
}

surveydata$R.level[14] <- "intermediate"

kable(surveydata, caption = "tidy survey data")

```

#Frequency graph & Word Cloud#

```{r echo=F,  message=F, warning=FALSE}
###This code grabs the above surveydata by columns and store the data in a vector, 'vect'. Then, it uses textmining functions such as unnest_tokens and anti_join to expand and clean the dataset. Then, uses ggplot to disply graphically the frequency of the words.

vect <- c()

for(i in 1:length(surveydata[1, ]))
{
  vect[i] <- paste(as.character(surveydata[ ,i]), sep = " ", collapse = " ")
}

text <- vect
text_df <- data_frame(line = 1:4, text = text)
word_df <- text_df %>% unnest_tokens(word, text) %>% anti_join(stop_words)
#frequency <- word_df %>% count(word, sort = TRUE) %>% mutate(proportion = n / sum(n))

word_df %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

ggtitle("Frequency graph")
```

<p> The graph displays the frequency of the words used in the dataset. It seems like 'pc' is the most common word found in the data followed by math, intermediate. I notice that there are more intermediate than beginner in r programming language while SAS is the most common word used for programming. </p>

```{r wordClould, echo = F,  message=F, warning=FALSE}
###This code creates a word cloud that displays the words in the dataset accroding to their frequency.

library(wordcloud)
word_df %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, min.freq = 0, random.order = F))

```

<p> This graph again shows the frequency of the words in the dataset, but in a word-cloud fasion. The term at the center with large letter is 'pc', which represents the most common word and we found it to be true also for the previous graph. The next biggest and most-centered word is 'intermediate', which corresponds with the above result. </p>


#Problem 3: Comparing frequency of words used in Genesis 1,2 of KJV, using Case study 7#
```{r echo = F,  message=F, warning=FALSE}
###This code compares the frequency of words used by Genesis1 and Genesis2 of KJV.
#Load data. Genesis1 vs. Genesis2

url<-"C:/Users/Administrator/Desktop/Bible(Genesis1,2).txt"
bibledata <- read.delim(url,  header=F, skip=0, fill=T, stringsAsFactors = F)
```

```{r echo = F, message=F, warning=FALSE}
###Finds frequency utilizing textmining code in case study 7.
library(tidyr)
library(dplyr)
library(tokenizers)
library(tidytext)

bible_df <- data_frame(id = c("Genesis1", "Genesis2"), text = bibledata$V1)
bible_df <- bible_df %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)

bible_pairs <- bible_df %>% 
  pairwise_count(word, id, sort = TRUE, upper = FALSE)

frequency <- bible_df %>% 
  group_by(id) %>% 
  count(word, sort = TRUE) %>% 
  left_join(bible_df %>% 
              group_by(id) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total)

frequency <- frequency %>% 
  select(id, word, freq) %>% 
  spread(id, freq) %>%
  arrange(Genesis1, Genesis2)
```

```{r echo = F,  message=F, warning=FALSE}
library(scales)
library(ggplot2)
ggplot(frequency, aes(Genesis1, Genesis2)) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  geom_abline(color = "blue")

#Comparing the frequency of words used by Genesis1 and Genesis2 of KJV.

```

#Problem 4#
Account Created!
